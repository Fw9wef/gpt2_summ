Этот репозиторий содержит код для обучения gpt2 для задачи резюмирования текста с помощью перекрестной энтропии.
Для обучения сети использовался датасет CNN/DM.
Помимо цикла обучения в репозитории находится код для подготовки данных.

# Подготовка данных для обучения
## Шаг 1. Загрузка данных
Датасет можно скачать тут https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail
Сеть можно обучить как данных CNN или DM по отдельности, так и на всех данных сразу.
Однако имеет значение как именно будет названа папка с данными.

Для обучения на CNN необходимо скачать архив по ссылке и распаковать содержимое в папку с названием, начинающимся на CNN (например, CNN_data).
Аналогично архив DM распаковывается в папку с названием на DM (DM_data).
Для обучения на совместном датасете нужно скачать оба архива и распаковать в одну и ту же папку с любым названием (не начинающимся ни на CNN, ни на DM).
Предположим, что мы хотим использовать весь датасет CNN/DM и распаковали файлы в папку all_data.

## Шаг 2. Находим длины текстов и резюме
В ходе шага будет получена общая статистика о количестве слов в каждом тексте и резюме.
Она будет сохранена в виде .png гистограммы в папке с данными. Запускаем скрипт max_article_sizes.py, в качестве аргумента указываем путь к распакованным данным.

`python max_article_sizes.py all_data`

Скрипт создаст новую папку (для датасета CNN это будет папка CNN, для датасета CNN/DM соответственно CNN-DM).
Внутри нее будет находится гистограмма и pickle-файл, содержащий длины текстов и пути к ним.

## Шаг 3. Записываем пары текст-резюме в json-файлы
При выполнении этого действия резюмируемый текст будет обрезан до первых 923 токенов, а резюме до первых 100.
Для выполнения необходимо запустить скрипт prepare_data.py и передать ему путь к picle-файлу, полученному на прошлом шаге:

`python prepare_data.py CNN-DM/CNN-DM_file_size.pickle`

В ходе выполнения скрипта внутри папки CNN-DM будет создана директория stories_tokenized (или cnn_stories_tokenized, сли изначально планировалось использовать только данные CNN).
В нее будут записаны json-файлы, содержащие текст ('article'), соответствующее резюме ('abstract') и порядковый номер пары ('id').
В конце работы скрип разделит весь датасет на трейн (80%), валидацию (10%) и тест (10%).
Принадлежность единицы данных к той или иной группе можно узнать из json-файла ids.json в папке CNN/DM. Ключи 'train_ids', 'valid_ids' и 'test_ids'.

# Запуск обучения модели
Для запуска обучения необходимо запустить скрипт train_gpt2_summarizer.py 

`python train_gpt2_summarizer.py --lr 5e-5 --batch_size 4 --n_gpus 2 --gradient_accumulation_step 2 
--num_train_epochs 5 --output_dir output --model_dir weights
--root_dir CNN-DM/stories_tokenized --ids_file CNN-DM/ids.json`

n_gpus - количество карт, которые будут задействованы для обучения  
gradient_accumulation_step - шаг оптимизатора будет происходить через каждые gradient_accumulation_step батчей  
output_dir - в этой папке будут созданы txt файлы с метриками  
model_dir - в эту папку будут сохраняться веса моделей  
root_dir - папка с датасетом в формате json. Если подготовить данные как в примере выше, то это будет
CNN-DM/stories_tokenized  
ids_file - путь к json файлу, содержащему разбиение датасета на трейн, валидацию и тест

# Тестирование модели
Для тестирования модели необходимо запустить скрипт test_model.py

`python test_model.py --output_dir output --model_dir weights/top_model_dir
--root_dir CNN-DM/stories_tokenized --ids_file CNN-DM/ids.json`
